# ğŸ“š DÃ­a 5 - SesiÃ³n 2: Machine Learning - Clustering K-Means

**Fecha:** 5 de febrero de 2026  
**Horario:** 19:00 - 21:00 (2 horas)  
**Tema:** Aprendizaje No Supervisado - K-Means en Profundidad

---

## ğŸ¯ Objetivos de la SesiÃ³n

âœ… Profundizar en clustering y sus aplicaciones  
âœ… Dominar K-Means: convergencia y funcionamiento interno  
âœ… Aprender mÃ©todos para elegir K Ã³ptimo  
âœ… Comprender Elbow Method y Silhouette Coefficient  
âœ… Explorar aplicaciones reales de aprendizaje no supervisado

---

## ğŸ“š Contenido Cubierto

### 1. Clustering - Concepto Fundamental

**Â¿QuÃ© resuelve?**
- Agrupa datos SIN etiquetas previas
- Encuentra patrones naturales automÃ¡ticamente
- Maximiza similitud DENTRO de grupos
- Minimiza similitud ENTRE grupos

**Tipos de distancia:**
- **Euclidiana:** LÃ­nea recta entre puntos (la mÃ¡s comÃºn)
- **Manhattan:** Distancia en calles perpendiculares
- **Coseno:** Ãngulo entre vectores (Ãºtil para texto)

### 2. K-Means - Algoritmo Detallado

**Proceso paso a paso:**
```
1. Elegir K (nÃºmero de clusters)
2. Inicializar K centroides (aleatorios o K-Means++)
3. REPETIR hasta convergencia:
   a) ASIGNACIÃ“N: Cada punto al centroide mÃ¡s cercano
   b) ACTUALIZACIÃ“N: Recalcular centroides (promedio)
   c) VERIFICAR: Â¿Centroides dejaron de moverse?
4. RETORNAR: Asignaciones y centroides finales
```

**Convergencia:**
- Se alcanza cuando centroides NO cambian
- O cuando cambio < tolerancia (ej: 0.0001)
- TÃ­picamente: 10-50 iteraciones

**ImplementaciÃ³n:**
- Desarrollamos versiÃ³n manual desde cero (educativa)
- Comparamos con sklearn (validaciÃ³n)
- Visualizamos evoluciÃ³n iteraciÃ³n por iteraciÃ³n

### 3. AsignaciÃ³n de Clusters - Proceso de PredicciÃ³n

**Regla fundamental:**
- Cada punto â†’ centroide MÃS CERCANO
- Distancia euclidiana: `âˆšÎ£(xi - ci)Â²`

**Ejemplo paso a paso:**
```python
Punto P = [3, 3]
Centroides:
  C1 = [2, 2]  â†’ distancia = 1.41
  C2 = [8, 8]  â†’ distancia = 7.07
  C3 = [2, 8]  â†’ distancia = 5.10

MÃ­nima: 1.41 â†’ Asignar a C1
```

**Confianza en asignaciÃ³n:**
- Alta: Muy cerca de su centroide, lejos de otros
- Baja: Distancias similares a mÃºltiples centroides
- Punto en frontera: Equidistante

### 4. Elbow Method (MÃ©todo del Codo)

**Concepto:**
- Prueba diferentes valores de K
- Calcula inercia (WCSS) para cada K
- Busca el "codo" en la curva

**Inercia (Within-Cluster Sum of Squares):**
- Suma de distanciasÂ² de cada punto a su centroide
- MENOR = mejor (clusters mÃ¡s compactos)
- Siempre decrece al aumentar K

**InterpretaciÃ³n:**
```
Antes del codo: Gran reducciÃ³n de inercia
En el codo: Punto de equilibrio (K Ã³ptimo)
DespuÃ©s del codo: Poca mejora adicional
```

**DetecciÃ³n automÃ¡tica:**
- Segunda derivada (mayor cambio de pendiente)
- MÃ©todo geomÃ©trico (distancia a lÃ­nea referencia)

**Limitaciones:**
- A veces no hay codo claro
- Puede haber mÃºltiples codos (ambiguo)
- Subjetivo (interpretaciÃ³n visual)

### 5. Silhouette Coefficient

**Â¿QuÃ© mide?**
- Calidad de asignaciÃ³n de cada punto
- Combina cohesiÃ³n (dentro) y separaciÃ³n (entre)

**FÃ³rmula para punto i:**
```
a(i) = distancia promedio a su mismo cluster
b(i) = distancia promedio al cluster MÃS CERCANO

s(i) = (b(i) - a(i)) / max(a(i), b(i))
```

**Rango de valores:**
```
+1.0 : PERFECTO - Muy dentro de su cluster
+0.7 : EXCELENTE - Clusters bien definidos
+0.5 : BUENO - Estructura razonable
+0.25: ACEPTABLE - Estructura dÃ©bil
 0.0 : FRONTERA - Entre clusters
-1.0 : MAL ASIGNADO - DeberÃ­a estar en otro
```

**Silhouette Score global:**
- Promedio de todos los puntos
- Para elegir K: MAXIMIZAR este valor
- MÃ¡s objetivo que Elbow Method

**Silhouette Diagram:**
- Visualiza silhouette de cada punto
- Por cluster y ordenado
- Identifica puntos mal asignados (negativos)

### 6. ComparaciÃ³n de MÃ©todos

**Elbow Method:**
- âœ… RÃ¡pido, fÃ¡cil de entender
- âŒ Subjetivo, a veces sin codo claro

**Silhouette Score:**
- âœ… Objetivo, mide cohesiÃ³n + separaciÃ³n
- âœ… Identifica puntos mal asignados
- âŒ MÃ¡s costoso computacionalmente

**RecomendaciÃ³n:**
- Usar AMBOS mÃ©todos
- Si coinciden â†’ alta confianza
- Considerar conocimiento del dominio

---

## ğŸš€ Aplicaciones Reales del Aprendizaje No Supervisado

### 1. **DetecciÃ³n de AnomalÃ­as**

**Fraude bancario:**
- PatrÃ³n normal: Compras 20-100â‚¬, ciudad local, horario diurno
- AnomalÃ­a: 5000â‚¬, otro paÃ­s, 3am, mÃºltiples transacciones
- Algoritmo: Isolation Forest
- Impacto: 95% de fraudes detectados antes de completarse

**Mantenimiento predictivo:**
- Sensores en maquinaria (temperatura, vibraciÃ³n, presiÃ³n)
- Detectar comportamiento anÃ³malo ANTES del fallo
- Impacto: 70% reducciÃ³n en paradas no planificadas

**Ciberseguridad:**
- Detectar ataques DDoS en tiempo real
- TrÃ¡fico anÃ³malo vs patrÃ³n normal
- Bloqueo automÃ¡tico de amenazas

### 2. **Sistemas de RecomendaciÃ³n**

**Filtrado colaborativo:**
- Netflix, Spotify, Amazon
- Agrupa usuarios con gustos similares
- "Si te gustÃ³ X, tambiÃ©n te gustarÃ¡ Y"
- Impacto: 80% del contenido consumido en Netflix

**Market Basket Analysis:**
- "Los clientes que compran X tambiÃ©n compran Y"
- OptimizaciÃ³n de disposiciÃ³n de productos

### 3. **SegmentaciÃ³n de Clientes**

**E-commerce:**
- Agrupa clientes sin categorÃ­as predefinidas
- Descubre segmentos: VIPs, Leales, Ocasionales, Dormidos
- CampaÃ±as de marketing personalizadas
- Impacto: +30% en conversiÃ³n

**Resultados tÃ­picos:**
- 5% VIPs â†’ 40% ingresos (atenciÃ³n premium)
- 25% Leales â†’ 35% ingresos (fidelizaciÃ³n)
- 50% Ocasionales â†’ 20% ingresos (activaciÃ³n)
- 20% Dormidos â†’ 5% ingresos (reactivaciÃ³n/descarte)

### 4. **Procesamiento de ImÃ¡genes**

**Medicina:**
- SegmentaciÃ³n de radiografÃ­as (tejidos, tumores)
- DetecciÃ³n temprana de cÃ¡ncer
- Clustering de intensidades de pÃ­xeles

**CompresiÃ³n:**
- Reducir colores: 16 millones â†’ 16 colores dominantes
- TamaÃ±o: 90% menor, calidad: ~95% similar

### 5. **Procesamiento de Lenguaje Natural**

**OrganizaciÃ³n de documentos:**
- Agrupa miles de documentos automÃ¡ticamente
- Google News: agrupa noticias similares
- Topic Modeling (LDA): Descubre temas ocultos

### 6. **Medicina y Salud**

**Descubrimiento de subtipos:**
- 5 subtipos de diabetes descubiertos con clustering (2018)
- Tratamientos personalizados por subtipo
- Mejora en eficacia de medicamentos

### 7. **Otras Aplicaciones**

- ğŸµ **Spotify:** GeneraciÃ³n de playlists (Discover Weekly)
- ğŸ™ï¸ **Urbanismo:** PlanificaciÃ³n de transporte pÃºblico
- ğŸ§¬ **GenÃ©tica:** Agrupar genes con funciones similares
- ğŸŒ¡ï¸ **Clima:** Descubrir patrones climÃ¡ticos no conocidos

---

## ğŸ’¡ ConexiÃ³n con Python y Proyectos Personales

### ML + Python = Superpoder

**Para THDORA (tu proyecto de hÃ¡bitos):**

1. **Clustering:**
   - Agrupar dÃ­as similares automÃ¡ticamente
   - "Lunes y martes son parecidos, viernes es diferente"

2. **DetecciÃ³n de anomalÃ­as:**
   - Identificar dÃ­as "raros" automÃ¡ticamente
   - "Hoy tu patrÃ³n es muy diferente, Â¿todo bien?"

3. **Pattern Discovery:**
   - Descubrir relaciones ocultas
   - "Cuando duermes poco + no desayunas â†’ baja productividad"
   - Sin que tÃº le digas explÃ­citamente quÃ© buscar

4. **PredicciÃ³n:**
   - Predecir productividad segÃºn hora/dÃ­a
   - Recomendar mejor momento para cada tarea
   - Estimar duraciÃ³n real de tareas

**Ventajas del ML:**
- âœ… NO necesitas etiquetar datos manualmente
- âœ… Descubre patrones OCULTOS
- âœ… Escalable a millones de datos
- âœ… Se adapta con nuevos datos
- âœ… AutomatizaciÃ³n inteligente

---

## ğŸ”§ Herramientas y CÃ³digo

### LibrerÃ­as principales:
```python
scikit-learn  # K-Means, mÃ©tricas, preprocesamiento
numpy         # Operaciones numÃ©ricas
pandas        # Manejo de datos
matplotlib    # VisualizaciÃ³n
```

### Implementaciones desarrolladas:
1. âœ… K-Means manual desde cero (educativo)
2. âœ… CÃ¡lculo de inercia paso a paso
3. âœ… Silhouette manual con explicaciones
4. âœ… Elbow Method con detecciÃ³n automÃ¡tica de codo
5. âœ… Silhouette Diagram completo
6. âœ… ComparaciÃ³n visual de mÃ©todos
7. âœ… VisualizaciÃ³n de convergencia iterativa

---

## ğŸ“Š Conceptos Clave Aprendidos

### K-Means:
- Algoritmo iterativo hasta convergencia
- AsignaciÃ³n basada en distancia mÃ­nima
- Sensible a inicializaciÃ³n (usar K-Means++)
- Mini-Batch K-Means para datasets grandes

### EvaluaciÃ³n de Clustering:
- **Inercia (WCSS):** Compacidad de clusters (menor mejor)
- **Silhouette Score:** Calidad de asignaciÃ³n [-1, +1] (mayor mejor)
- **Davies-Bouldin Index:** SeparaciÃ³n entre clusters (menor mejor)
- **Calinski-Harabasz:** Ratio varianza inter/intra (mayor mejor)

### Elegir K Ã³ptimo:
1. Elbow Method â†’ Buscar "codo" visual
2. Silhouette Score â†’ Maximizar
3. Davies-Bouldin â†’ Minimizar
4. Combinar mÃºltiples mÃ©tricas
5. Considerar conocimiento del dominio

---

## ğŸ¯ Plan de ImplementaciÃ³n Futura

### Para sistema de hÃ¡bitos/horarios con ML:

**Fase 1: RecolecciÃ³n (Mes 1)**
- Registrar todas las sesiones/tareas
- MÃ­nimo 60 registros antes de entrenar
- Solo acumular datos, sin ML

**Fase 2: Primeros Modelos (Mes 2)**
- Entrenar modelo de clustering (patrones de dÃ­as)
- Modelo de predicciÃ³n de productividad
- Validar con mÃ©tricas

**Fase 3: Refinamiento (Mes 3+)**
- Re-entrenar con mÃ¡s datos
- Ajustar hiperparÃ¡metros
- OptimizaciÃ³n automÃ¡tica de horarios

**Datos a recolectar:**
```python
{
    "fecha": "2026-02-05",
    "hora_inicio": 10,
    "dia_semana": 3,
    "tipo_tarea": "estudiar",
    "duracion_planificada": 120,
    "horas_sueno_previas": 7,
    "energia_inicial": 7,
    "completada": True,
    "duracion_real": 135,
    "productividad": 8
}
```

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… Documentar toda la sesiÃ³n (este archivo)
2. â³ Actualizar apuntes en repo personal
3. â³ SesiÃ³n THDORA (1 hora) - Aplicar conceptos aprendidos
4. â³ Implementar sistema de registro bÃ¡sico para acumular datos

---

## ğŸ“ ReflexiÃ³n

**Lo mÃ¡s importante de hoy:**
- Machine Learning NO supervisado es increÃ­blemente poderoso
- No necesita datos etiquetados (CARO de obtener)
- Clustering encuentra patrones que humanos no ven
- Aplicaciones en TODOS los campos: finanzas, salud, marketing, etc.
- Python + ML = DiferenciaciÃ³n profesional masiva

**Siguiente objetivo:**
- Aplicar estos conceptos a THDORA
- Empezar a recolectar datos estructurados
- En 30 dÃ­as: primer modelo entrenado

---

## â° Registro de Tiempo

- **19:00 - 19:30:** Clustering general y K-Means fundamentals
- **19:30 - 20:00:** Convergencia, asignaciÃ³n, predicciÃ³n
- **20:00 - 20:30:** Elbow Method y elegir K Ã³ptimo
- **20:30 - 20:45:** Silhouette Coefficient profundo
- **20:45 - 21:00:** Aplicaciones reales ML no supervisado

**DuraciÃ³n total:** 2 horas  
**Intensidad:** Alta - Muchos conceptos nuevos  
**ComprensiÃ³n:** 85% - Necesito practicar implementaciones

---

## ğŸ“š Recursos para Revisar

- Implementaciones manuales de algoritmos
- Visualizaciones de convergencia
- Ejemplos de aplicaciones reales
- CÃ³digo de detecciÃ³n de anomalÃ­as
- Plan de implementaciÃ³n THDORA con ML

---

**Estado:** âœ… SesiÃ³n completada  
**Siguiente:** ğŸš€ 1 hora THDORA - Aplicar conceptos

---

*Generado el 5 de febrero de 2026 a las 21:00*